{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab267d60",
        "outputId": "e33b1ba0-5706-4bff-ee63-2b6528568acc",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.1\n",
            "Cloning into 'pyspellchecker'...\n",
            "remote: Enumerating objects: 712, done.\u001b[K\n",
            "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 712 (delta 33), reused 61 (delta 24), pack-reused 633\u001b[K\n",
            "Receiving objects: 100% (712/712), 68.53 MiB | 19.58 MiB/s, done.\n",
            "Resolving deltas: 100% (385/385), done.\n",
            "/usr/bin/python3: No module named build\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
            "Installing collected packages: tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.19.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lazypredict\n",
            "  Downloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from lazypredict) (1.2.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.8/dist-packages (from lazypredict) (0.90)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from lazypredict) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from lazypredict) (1.0.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.8/dist-packages (from lazypredict) (2.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from lazypredict) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from lazypredict) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from lightgbm->lazypredict) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from lightgbm->lazypredict) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->lazypredict) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->lazypredict) (2022.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->lazypredict) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->lazypredict) (1.15.0)\n",
            "Installing collected packages: lazypredict\n",
            "Successfully installed lazypredict-0.2.12\n",
            "/bin/bash: -c: line 0: syntax error near unexpected token `'stopwords''\n",
            "/bin/bash: -c: line 0: `nltk.download('stopwords')'\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting translate\n",
            "  Downloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\n",
            "Collecting libretranslatepy==2.1.1\n",
            "  Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from translate) (2.25.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from translate) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from translate) (4.9.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->translate) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->translate) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->translate) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->translate) (2022.12.7)\n",
            "Installing collected packages: libretranslatepy, translate\n",
            "Successfully installed libretranslatepy-2.1.1 translate-3.6.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.3-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=3129581 sha256=a51dfe798d73f6c366b9795f5c78be60433f24334286e0c1f33e4ffcc8fb166c\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.3\n"
          ]
        }
      ],
      "source": [
        "#!pip install --upgrade google-cloud-speech\n",
        "#-----------------------------------------------\n",
        "!pip install pyspellchecker\n",
        "!git clone https://github.com/barrust/pyspellchecker.git\n",
        "!cd pyspellchecker\n",
        "!python -m build\n",
        "#-----------------------------------------------\n",
        "! pip install -q tensorflow-text\n",
        "! pip install tensorflow_addons\n",
        "! pip install transformers\n",
        "#-----------------------------------------------\n",
        "!pip install lazypredict\n",
        "#-----------------------------------------------\n",
        "!nltk.download('stopwords')\n",
        "#-----------------------------------------------\n",
        "!pip install translate\n",
        "#-----------------------------------------------\n",
        "!pip install fasttext"
      ],
      "id": "ab267d60"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7ebaea2",
        "outputId": "84bc5f06-df18-4e8f-9539-8aeb46583fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "#from google.cloud import speech\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "#-----------------------------------------------\n",
        "from spellchecker import SpellChecker\n",
        "#-----------------------------------------------\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_addons as tfa\n",
        "import keras\n",
        "from keras import backend as k\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation\n",
        "from keras.layers.core import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import categorical_crossentropy\n",
        "\n",
        "#-----------------------------------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "#-----------------------------------------------\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import NuSVC\n",
        "from sklearn import svm\n",
        "import lightgbm as lgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import f1_score\n",
        "#-----------------------------------------------\n",
        "from lazypredict.Supervised import LazyClassifier\n",
        "from translate import Translator\n",
        "#-----------------------------------------------\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "#-----------------------------------------------\n",
        "import fasttext"
      ],
      "id": "c7ebaea2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voAwDR1jQf3b",
        "outputId": "5c923c4e-bc7e-440f-f270-4707bdbc96e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "id": "voAwDR1jQf3b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B4UOMagJMAV"
      },
      "source": [
        "# Choose Question"
      ],
      "id": "4B4UOMagJMAV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ3UmnMBJO3k"
      },
      "outputs": [],
      "source": [
        "question=1"
      ],
      "id": "cJ3UmnMBJO3k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing functions\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KklHuc2le1fF"
      },
      "id": "KklHuc2le1fF"
    },
    {
      "cell_type": "code",
      "source": [
        "num2words = {1: 'One', 2: 'Two', 3: 'Three', 4: 'Four', 5: 'Five', \\\n",
        "             6: 'Six', 7: 'Seven', 8: 'Eight', 9: 'Nine', 10: 'Ten', \\\n",
        "            11: 'Eleven', 12: 'Twelve', 13: 'Thirteen', 14: 'Fourteen', \\\n",
        "            15: 'Fifteen', 16: 'Sixteen', 17: 'Seventeen', 18: 'Eighteen', \\\n",
        "            19: 'Nineteen', 20: 'Twenty', 30: 'Thirty', 40: 'Forty', \\\n",
        "            50: 'Fifty', 60: 'Sixty', 70: 'Seventy', 80: 'Eighty', \\\n",
        "            90: 'Ninety', 0: 'Zero'}\n",
        "\n",
        "def n2w(n):\n",
        "  try:\n",
        "      return num2words[n]\n",
        "  except KeyError:\n",
        "      try:\n",
        "          return (num2words[n-n%10] +' '+ num2words[n%10].lower())\n",
        "      except KeyError:\n",
        "          return '-99'\n",
        "\n",
        "def remove_extra_space(text):\n",
        "    return re.sub(' +', ' ', text)\n",
        "\n",
        "punctuation = string.punctuation + \"؟'«»٫٬،.؛!()-[]{};:\\,<>./?@$%^&*_~\"\n",
        "def remove_punctuation(text):\n",
        "    #punctuation = string.punctuation + \"؟'«»٫٬،.؛\"\n",
        "    return text.translate(str.maketrans('', '', punctuation))\n",
        "\n",
        "translator= Translator(to_lang=\"Arabic\")\n",
        "def change_numbers(text):\n",
        "  try:\n",
        "    text_original=text\n",
        "    text=text.split()\n",
        "    for i in range (len(text)):\n",
        "      if(text[i].isnumeric()):\n",
        "        english=n2w(int(text[i]))\n",
        "        translation = translator.translate(english)\n",
        "        text[i]=translation\n",
        "    text=\" \".join(text)\n",
        "    return text\n",
        "  except:\n",
        "    return text_original\n",
        "\n",
        "spell = SpellChecker(language='ar')   # use the Arabic Dictionary\n",
        "def separate_waw(text):\n",
        "  splitted_text=text.split()\n",
        "  for j in range(0,len(splitted_text)): \n",
        "      if((splitted_text[j][0]=='و') and (splitted_text[j]!=spell.correction(splitted_text[j]))):\n",
        "              splitted_text[j] = splitted_text[j][:0+1] + ' ' + splitted_text[j][0+1:]\n",
        "  text=\" \".join(splitted_text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "HprpDbmOe5E8"
      },
      "id": "HprpDbmOe5E8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09c1507f"
      },
      "source": [
        "# Reference Dataset"
      ],
      "id": "09c1507f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72ae4225"
      },
      "outputs": [],
      "source": [
        "ref_df=pd.read_csv('/content/drive/MyDrive/Speech Dataset/sas-ar.csv')\n",
        "ref_df=ref_df[ref_df['EssaySet']==question]\n",
        "ref_df.reset_index(inplace=True)"
      ],
      "id": "72ae4225"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "475d46eb"
      },
      "outputs": [],
      "source": [
        "#Reference Dataset Preprocessing\n",
        "\n",
        "ref_df[\"EssayText\"] = ref_df[\"EssayText\"].apply(lambda text: remove_punctuation(text))\n",
        "ref_df[\"EssayText\"] = ref_df[\"EssayText\"].apply(lambda text: remove_extra_space(text))\n",
        "#ref_df[\"EssayText\"] = ref_df[\"EssayText\"].apply(lambda text: change_numbers(text))\n",
        "ref_df[\"EssayText\"] = ref_df[\"EssayText\"].apply(lambda text: separate_waw(text))\n",
        "refs=ref_df['EssayText']\n",
        "refs=np.array(refs)\n",
        "\n",
        "#Exchaning similar letters \n",
        "\n",
        "for i in range(0,len(refs)):\n",
        "    ref=refs[i]\n",
        "    #ref = ref.translate({ord(''): ''}) \n",
        "    ref = ref.translate({ord('ة'): 'ه'})\n",
        "    ref = ref.translate({ord('ً'): None})\n",
        "    ref = ref.translate({ord('أ'): 'ا'}) \n",
        "    ref = ref.translate({ord('إ'): 'ا'}) \n",
        "    refs[i]=ref\n",
        "\n",
        "ref_df[\"EssayText\"]=refs\n",
        "    \n",
        "\n",
        "#Exchaning letters with a manual approach \n",
        "    \n",
        "# for i in range(0,len(refs)):\n",
        "#     ref=refs[i]\n",
        "#     ref=list(ref)\n",
        "#     for j in range(0,len(ref)):\n",
        "#         if(ref[j]=='ة'):\n",
        "#             ref[j]='ه'\n",
        "#         if(ref[j]==' ً'):\n",
        "#             ref=ref[0 : j : ] + ref[j + 1 : :]\n",
        "#         ref_joined=\"\".join(ref)\n",
        "#     refs[i]=ref_joined"
      ],
      "id": "475d46eb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHidDT-KVieL"
      },
      "source": [
        "# Converted Dataset"
      ],
      "id": "bHidDT-KVieL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hjiUjvxVnsV"
      },
      "outputs": [],
      "source": [],
      "id": "4hjiUjvxVnsV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rCf_ySlY5zO"
      },
      "outputs": [],
      "source": [
        "#Dropping nulls from df and ref_df\n",
        "null_df=df.isnull()\n",
        "nulls_index=null_df[null_df['EssayText']==True].index\n",
        "for x in nulls_index:\n",
        "  #ref_df.drop(x,axis=0, inplace=True)\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "df.reset_index(inplace=True)\n",
        "ref_df.reset_index(inplace=True)"
      ],
      "id": "1rCf_ySlY5zO"
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Preprocessing\n",
        "\n",
        "df[\"EssayText\"] = df[\"EssayText\"].apply(lambda text: separate_waw(text))\n",
        "df[\"EssayText\"] = df[\"EssayText\"].apply(lambda text: remove_punctuation(text))\n",
        "df[\"EssayText\"] = df[\"EssayText\"].apply(lambda text: remove_extra_space(text))"
      ],
      "metadata": {
        "id": "LInE2HqFfhvt"
      },
      "id": "LInE2HqFfhvt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a09a8932"
      },
      "source": [
        "# Conversion Accuracy \"Word Error Rate\""
      ],
      "id": "a09a8932"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "543c7c2f"
      },
      "outputs": [],
      "source": [
        "def wer(ref, hyp ,debug=True):\n",
        "    r = ref.split()\n",
        "    h = hyp.split()\n",
        "    #costs will holds the costs, like in the Levenshtein distance algorithm\n",
        "    costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
        "    # backtrace will hold the operations we've done.\n",
        "    # so we could later backtrace, like the WER algorithm requires us to.\n",
        "    backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
        " \n",
        "    OP_OK = 0\n",
        "    OP_SUB = 1\n",
        "    OP_INS = 2\n",
        "    OP_DEL = 3\n",
        "    DEL_PENALTY = 1\n",
        "    INS_PENALTY = 1\n",
        "    SUB_PENALTY = 1\n",
        "    \n",
        "    # First column represents the case where we achieve zero\n",
        "    # hypothesis words by deleting all reference words.\n",
        "    for i in range(1, len(r)+1):\n",
        "        costs[i][0] = DEL_PENALTY*i\n",
        "        backtrace[i][0] = OP_DEL\n",
        "    \n",
        "    # First row represents the case where we achieve the hypothesis\n",
        "    # by inserting all hypothesis words into a zero-length reference.\n",
        "    for j in range(1, len(h) + 1):\n",
        "        costs[0][j] = INS_PENALTY * j\n",
        "        backtrace[0][j] = OP_INS\n",
        "    \n",
        "    # computation\n",
        "    for i in range(1, len(r)+1):\n",
        "        for j in range(1, len(h)+1):\n",
        "            if r[i-1] == h[j-1]:\n",
        "                costs[i][j] = costs[i-1][j-1]\n",
        "                backtrace[i][j] = OP_OK\n",
        "            else:\n",
        "                substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1\n",
        "                insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1\n",
        "                deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1\n",
        "                 \n",
        "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
        "                if costs[i][j] == substitutionCost:\n",
        "                    backtrace[i][j] = OP_SUB\n",
        "                elif costs[i][j] == insertionCost:\n",
        "                    backtrace[i][j] = OP_INS\n",
        "                else:\n",
        "                    backtrace[i][j] = OP_DEL\n",
        "                 \n",
        "    # back trace though the best route:\n",
        "    i = len(r)\n",
        "    j = len(h)\n",
        "    numSub = 0\n",
        "    numDel = 0\n",
        "    numIns = 0\n",
        "    numCor = 0\n",
        "    if debug:\n",
        "        print(\"OP\\tREF\\tHYP\")\n",
        "        lines = []\n",
        "    while i > 0 or j > 0:\n",
        "        if backtrace[i][j] == OP_OK:\n",
        "            numCor += 1\n",
        "            i-=1\n",
        "            j-=1\n",
        "            if debug:\n",
        "                lines.append(\"OK\\t\" + r[i]+\"\\t\"+h[j])\n",
        "        elif backtrace[i][j] == OP_SUB:\n",
        "            numSub +=1\n",
        "            i-=1\n",
        "            j-=1\n",
        "            if debug:\n",
        "                lines.append(\"SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
        "        elif backtrace[i][j] == OP_INS:\n",
        "            numIns += 1\n",
        "            j-=1\n",
        "            if debug:\n",
        "                lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
        "        elif backtrace[i][j] == OP_DEL:\n",
        "            numDel += 1\n",
        "            i-=1\n",
        "            if debug:\n",
        "                lines.append(\"DEL\\t\" + r[i]+\"\\t\"+\"****\")\n",
        "    if debug:\n",
        "        lines = reversed(lines)\n",
        "        for line in lines:\n",
        "            print(line)\n",
        "        print(\"#cor \" + str(numCor))\n",
        "        print(\"#sub \" + str(numSub))\n",
        "        print(\"#del \" + str(numDel))\n",
        "        print(\"#ins \" + str(numIns))\n",
        "    # return (numSub + numDel + numIns) / (float) (len(r))\n",
        "    wer_result = round( (numSub + numDel + numIns) / (float) (len(r)), 3)\n",
        "    return {'WER':wer_result, 'numCor':numCor, 'numSub':numSub, 'numIns':numIns, 'numDel':numDel, \"numCount\": len(r)}\n",
        " \n",
        "def wers(refs, hyps):\n",
        "    numSub = 0\n",
        "    numDel = 0\n",
        "    numCor = 0\n",
        "    numIns = 0\n",
        "    numCount = 0\n",
        "    for ref, hyp in zip(refs, hyps):\n",
        "        result = wer(ref, hyp, False)\n",
        "        numSub += result[\"numSub\"]\n",
        "        numDel += result[\"numDel\"]\n",
        "        numCor += result[\"numCor\"]\n",
        "        numIns += result[\"numIns\"]\n",
        "        numCount += result[\"numCount\"]\n",
        "    return round( (numSub + numDel + numIns) / (float) (numCount), 3)"
      ],
      "id": "543c7c2f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "200b8931"
      },
      "outputs": [],
      "source": [
        "wers(ref_df['EssayText'],df['EssayText'])"
      ],
      "id": "200b8931"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1WLmQjDn25k"
      },
      "source": [
        "# Embeddings/Lemma\n"
      ],
      "id": "h1WLmQjDn25k"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOI7WZ0Avupj"
      },
      "outputs": [],
      "source": [
        "# !pip install farasapy\n",
        "# from farasa.stemmer import FarasaStemmer\n",
        "# !pip install -U sentence-transformers\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# import nltk\n",
        "# from nltk.stem.isri import ISRIStemmer"
      ],
      "id": "lOI7WZ0Avupj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd1VvLWNJzJe"
      },
      "outputs": [],
      "source": [
        "# !conda create -n env_pytorch python=3.6\n",
        "# !conda activate env_pytorch\n",
        "# !pip install torchvision \n",
        "# import torch\n",
        "# import torchvision"
      ],
      "id": "pd1VvLWNJzJe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk__vrzWaDFi"
      },
      "outputs": [],
      "source": [
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
        "def embed_text_use(df):\n",
        "  X = np.array(embed(df['EssayText'])) \n",
        "  y = np.array(df[\"Score1\"])\n",
        "  return X,y\n",
        "\n",
        "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "def embed_text_BERT(df):\n",
        "  embeddings = model.encode(df['EssayText'])\n",
        "  X = np.array(embeddings) \n",
        "  y = np.array(df[\"Score1\"])\n",
        "  return X,y\n",
        "\n",
        "\n",
        "#ft= fasttext.load_model('/content/drive/My Drive/Speech Dataset/cc.ar.300.bin')\n",
        "def embed_fasttext2(sentence):\n",
        "  return ft.get_sentence_vector(sentence)\n",
        "\n",
        "def embed_fasttext(df):\n",
        "  df[\"vector\"] = df[\"EssayText\"].apply(embed_fasttext2)\n",
        "  X=[]\n",
        "  for i in range(len(df[\"EssayText\"])):\n",
        "    X.append(df[\"vector\"][i])\n",
        "  X=np.array(X)\n",
        "  y=np.array(df[\"Score1\"])\n",
        "  return X,y\n",
        "\n",
        "#stop_words = set(stopwords.words('arabic'))\n",
        "def removing_stop_words(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_sentence = []  \n",
        "  for w in word_tokens:\n",
        "      if w not in stop_words:\n",
        "          filtered_sentence.append(w)\n",
        "  text=\" \".join(filtered_sentence)\n",
        "  return text\n",
        "\n",
        "def lemmatization(df):\n",
        "  #st = ISRIStemmer()\n",
        "  fs = FarasaStemmer()\n",
        "  corpus = []\n",
        "  messages=df[\"EssayText\"]\n",
        "  for i in range(0, len(messages)):\n",
        "      message = messages[i]\n",
        "      stemmed_text = fs.stem(message)  \n",
        "      corpus.append(stemmed_text)\n",
        "  return corpus"
      ],
      "id": "Sk__vrzWaDFi"
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "\n",
        "lemma=lemmatization(df)\n",
        "df['EssayText']=lemma"
      ],
      "metadata": {
        "id": "q5LyDaziA12G"
      },
      "id": "q5LyDaziA12G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQNnrWOTaFHJ"
      },
      "outputs": [],
      "source": [
        "df[\"EssayText\"] = df[\"EssayText\"].apply(lambda text: change_numbers(text))\n",
        "#df[\"EssayText\"] = df[\"EssayText\"].apply(lambda text: removing_stop_words(text))\n",
        "\n",
        "X,y = embed_text_use(df)\n",
        "#X,y = embed_text_BERT(df)\n",
        "#X,y =embed_fasttext(df)"
      ],
      "id": "qQNnrWOTaFHJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jagcj1qu10t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aad2406-175a-45f1-9444-200e66b1b4e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([0, 1, 2]), array([231, 614, 427]))\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y ,test_size=0.2, random_state=42)\n",
        "num_labels=len(np.unique(y_train))\n",
        "print(np.unique(y_train, return_counts=True))"
      ],
      "id": "5Jagcj1qu10t"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4_IY8Zj2dk"
      },
      "source": [
        "# Before Deep Learning"
      ],
      "id": "8d4_IY8Zj2dk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2o-QqJIEG3pl"
      },
      "outputs": [],
      "source": [
        "X_train=np.expand_dims(X_train,axis=1)\n",
        "X_test=np.expand_dims(X_test,axis=1)"
      ],
      "id": "2o-QqJIEG3pl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bqW6e-QziU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b7e1993-e744-4950-c7b1-bd8026306803"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1272, 1, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "X_train.shape"
      ],
      "id": "3bqW6e-QziU-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM0L3PcAygBo"
      },
      "source": [
        "# LSTM"
      ],
      "id": "RM0L3PcAygBo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3UpPMzwyhdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0224e2d1-8f73-4c84-e222-cacb21bac082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "128/128 [==============================] - 9s 31ms/step - loss: 0.9122 - cohen_kappa: 0.2900 - accuracy: 0.5919 - val_loss: 0.7013 - val_cohen_kappa: 0.5854 - val_accuracy: 0.7137\n",
            "Epoch 2/7\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 0.7058 - cohen_kappa: 0.5279 - accuracy: 0.6991 - val_loss: 0.6570 - val_cohen_kappa: 0.6809 - val_accuracy: 0.7569\n",
            "Epoch 3/7\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 0.6286 - cohen_kappa: 0.6248 - accuracy: 0.7434 - val_loss: 0.6265 - val_cohen_kappa: 0.7124 - val_accuracy: 0.7608\n",
            "Epoch 4/7\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 0.5899 - cohen_kappa: 0.6567 - accuracy: 0.7532 - val_loss: 0.6064 - val_cohen_kappa: 0.7017 - val_accuracy: 0.7725\n",
            "Epoch 5/7\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 0.5638 - cohen_kappa: 0.6825 - accuracy: 0.7679 - val_loss: 0.5945 - val_cohen_kappa: 0.7297 - val_accuracy: 0.7882\n",
            "Epoch 6/7\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 0.5395 - cohen_kappa: 0.6931 - accuracy: 0.7837 - val_loss: 0.6116 - val_cohen_kappa: 0.7260 - val_accuracy: 0.7804\n",
            "Epoch 7/7\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 0.5212 - cohen_kappa: 0.7190 - accuracy: 0.7876 - val_loss: 0.6082 - val_cohen_kappa: 0.7184 - val_accuracy: 0.7843\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "epochs = 7\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
        "lstm_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(1,X_train.shape[2])),\n",
        "        tf.keras.layers.LSTM(128,activation=\"relu\", return_sequences=True),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.LSTM(64 ,activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(num_labels, activation=\"softmax\")\n",
        "])\n",
        "op = keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "lstm_model.compile(optimizer=op,\n",
        "              loss=loss,\n",
        "              metrics=[tfa.metrics.CohenKappa(weightage=\"quadratic\", sparse_labels=True, num_classes=num_labels),\"accuracy\"])\n",
        "lstm_history = lstm_model.fit(X_train, y_train,epochs=epochs, validation_split=0.2, batch_size=8, callbacks=[callback], shuffle=True)"
      ],
      "id": "l3UpPMzwyhdR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYtbu6FWy5b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394b19d7-9ea2-40a6-ea81-1dd1bddaac62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 8ms/step - loss: 0.7148 - cohen_kappa: 0.6248 - accuracy: 0.6865\n",
            "[0.7147642970085144, 0.6247808933258057, 0.6865203976631165]\n"
          ]
        }
      ],
      "source": [
        "print(lstm_model.evaluate(X_test,y_test))"
      ],
      "id": "EYtbu6FWy5b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwGYpVBAUWzn"
      },
      "source": [
        "# RNN"
      ],
      "id": "GwGYpVBAUWzn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85dv0eY9UZzQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40ae530d-6cde-45a7-ba26-22cab2905e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "143/143 [==============================] - 3s 9ms/step - loss: 0.8297 - cohen_kappa: 0.4460 - accuracy: 0.6477 - val_loss: 0.6337 - val_cohen_kappa: 0.7016 - val_accuracy: 0.7969\n",
            "Epoch 2/7\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.6516 - cohen_kappa: 0.6412 - accuracy: 0.7325 - val_loss: 0.5850 - val_cohen_kappa: 0.6588 - val_accuracy: 0.7891\n",
            "Epoch 3/7\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.5897 - cohen_kappa: 0.6856 - accuracy: 0.7692 - val_loss: 0.6528 - val_cohen_kappa: 0.6623 - val_accuracy: 0.7500\n",
            "Epoch 4/7\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.5420 - cohen_kappa: 0.7100 - accuracy: 0.7876 - val_loss: 0.6432 - val_cohen_kappa: 0.6638 - val_accuracy: 0.7578\n",
            "Epoch 5/7\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.5046 - cohen_kappa: 0.7399 - accuracy: 0.8059 - val_loss: 0.6306 - val_cohen_kappa: 0.6839 - val_accuracy: 0.7500\n",
            "Epoch 6/7\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.4931 - cohen_kappa: 0.7515 - accuracy: 0.8156 - val_loss: 0.6550 - val_cohen_kappa: 0.7227 - val_accuracy: 0.7891\n",
            "Epoch 7/7\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.4401 - cohen_kappa: 0.7807 - accuracy: 0.8322 - val_loss: 0.6997 - val_cohen_kappa: 0.7096 - val_accuracy: 0.7734\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
        "epoch = 7\n",
        "rnn_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(1,X_train.shape[2])),\n",
        "        tf.keras.layers.SimpleRNN(128,activation=\"relu\", return_sequences=True),\n",
        "        tf.keras.layers.Dropout(.2),\n",
        "        tf.keras.layers.SimpleRNN(64 ,activation=\"relu\"),\n",
        "        #tf.keras.layers.Dropout(.2),\n",
        "        tf.keras.layers.Dense(num_labels, activation=\"softmax\"),\n",
        "])\n",
        "op = keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "rnn_model.compile(optimizer=op,\n",
        "              loss=loss,\n",
        "              metrics=[tfa.metrics.CohenKappa(num_classes=num_labels, sparse_labels=True, weightage=\"quadratic\"),\"accuracy\"])\n",
        "rnn_history = rnn_model.fit(X_train, y_train,epochs=epoch, validation_split=0.1, batch_size=8, callbacks=[callback])"
      ],
      "id": "85dv0eY9UZzQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-BUwL02Uaqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3baa8574-0afb-4c0b-f4b0-eb78a7ea3910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 1s 4ms/step - loss: 0.7266 - cohen_kappa: 0.6464 - accuracy: 0.7085\n",
            "[0.7266185879707336, 0.6463850140571594, 0.7084639668464661]\n"
          ]
        }
      ],
      "source": [
        "print(rnn_model.evaluate(X_test,y_test))"
      ],
      "id": "0-BUwL02Uaqg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckT842vQUXM_"
      },
      "source": [
        "# Bi-directional"
      ],
      "id": "ckT842vQUXM_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3WGvlk6UhRf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f41a55-b986-47b6-bc2d-e2021c327a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "143/143 [==============================] - 17s 75ms/step - loss: 0.8612 - cohen_kappa: 0.3847 - accuracy: 0.6171 - val_loss: 0.6428 - val_cohen_kappa: 0.6099 - val_accuracy: 0.7344\n",
            "Epoch 2/5\n",
            "143/143 [==============================] - 9s 64ms/step - loss: 0.6647 - cohen_kappa: 0.6066 - accuracy: 0.7238 - val_loss: 0.6089 - val_cohen_kappa: 0.6856 - val_accuracy: 0.7891\n",
            "Epoch 3/5\n",
            "143/143 [==============================] - 9s 64ms/step - loss: 0.6019 - cohen_kappa: 0.6588 - accuracy: 0.7430 - val_loss: 0.5742 - val_cohen_kappa: 0.7329 - val_accuracy: 0.8125\n",
            "Epoch 4/5\n",
            "143/143 [==============================] - 10s 67ms/step - loss: 0.5688 - cohen_kappa: 0.7009 - accuracy: 0.7701 - val_loss: 0.6075 - val_cohen_kappa: 0.7316 - val_accuracy: 0.7812\n",
            "Epoch 5/5\n",
            "143/143 [==============================] - 9s 63ms/step - loss: 0.5372 - cohen_kappa: 0.7022 - accuracy: 0.7736 - val_loss: 0.5765 - val_cohen_kappa: 0.7351 - val_accuracy: 0.7891\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "tf.keras.backend.clear_session()\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
        "bi_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(1,X_train.shape[2])),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,activation=\"relu\", return_sequences=True)),\n",
        "        tf.keras.layers.Dropout(.2),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64 ,activation=\"relu\")),\n",
        "        #tf.keras.layers.Dropout(.2),\n",
        "        tf.keras.layers.Dense(num_labels, activation=\"softmax\"),\n",
        "])\n",
        "op = keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "bi_model.compile(optimizer=op,\n",
        "              loss=loss,\n",
        "              metrics=[tfa.metrics.CohenKappa(weightage=\"quadratic\", sparse_labels=True, num_classes=num_labels),\"accuracy\"])\n",
        "bi_history = bi_model.fit(X_train, y_train,epochs=epochs, validation_split=0.1, batch_size=8, shuffle=True, callbacks=[callback])"
      ],
      "id": "D3WGvlk6UhRf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBDw6j__UhN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3654a65e-52d3-43bc-c9f2-62c8af9a1373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 2s 86ms/step - loss: 0.6895 - cohen_kappa: 0.6643 - accuracy: 0.7085\n",
            "[0.6895091533660889, 0.6642712354660034, 0.7084639668464661]\n"
          ]
        }
      ],
      "source": [
        "print(bi_model.evaluate(X_test,y_test))"
      ],
      "id": "JBDw6j__UhN3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAlXVr-7Um1Y"
      },
      "source": [
        "# Before Machine Learning"
      ],
      "id": "VAlXVr-7Um1Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "detFMHz_FWj7"
      },
      "outputs": [],
      "source": [
        "X_train=X_train.reshape(X_train.shape[0],X_train.shape[2])\n",
        "X_test=X_test.reshape(X_test.shape[0],X_test.shape[2])\n"
      ],
      "id": "detFMHz_FWj7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW1orBt7GBza"
      },
      "source": [
        "# Lazy Predict"
      ],
      "id": "SW1orBt7GBza"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J97NymFGAmL",
        "outputId": "1a0a8663-68ec-483d-a447-4e450d8da812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29/29 [01:18<00:00,  2.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\n",
            "Model                                                                          \n",
            "NuSVC                              0.71               0.70    None      0.71   \n",
            "SVC                                0.71               0.69    None      0.71   \n",
            "LGBMClassifier                     0.69               0.69    None      0.69   \n",
            "XGBClassifier                      0.70               0.69    None      0.70   \n",
            "RandomForestClassifier             0.68               0.66    None      0.68   \n",
            "ExtraTreesClassifier               0.67               0.65    None      0.67   \n",
            "AdaBoostClassifier                 0.65               0.64    None      0.65   \n",
            "NearestCentroid                    0.62               0.64    None      0.60   \n",
            "SGDClassifier                      0.64               0.64    None      0.64   \n",
            "RidgeClassifierCV                  0.64               0.63    None      0.63   \n",
            "BernoulliNB                        0.60               0.63    None      0.58   \n",
            "LinearDiscriminantAnalysis         0.63               0.63    None      0.63   \n",
            "GaussianNB                         0.59               0.62    None      0.58   \n",
            "PassiveAggressiveClassifier        0.62               0.62    None      0.61   \n",
            "LogisticRegression                 0.60               0.60    None      0.60   \n",
            "RidgeClassifier                    0.61               0.60    None      0.61   \n",
            "Perceptron                         0.60               0.60    None      0.60   \n",
            "BaggingClassifier                  0.61               0.59    None      0.61   \n",
            "KNeighborsClassifier               0.60               0.59    None      0.59   \n",
            "CalibratedClassifierCV             0.61               0.59    None      0.61   \n",
            "LinearSVC                          0.57               0.58    None      0.57   \n",
            "DecisionTreeClassifier             0.54               0.53    None      0.54   \n",
            "ExtraTreeClassifier                0.51               0.52    None      0.51   \n",
            "QuadraticDiscriminantAnalysis      0.43               0.33    None      0.26   \n",
            "DummyClassifier                    0.43               0.33    None      0.26   \n",
            "LabelSpreading                     0.25               0.33    None      0.10   \n",
            "LabelPropagation                   0.25               0.33    None      0.10   \n",
            "\n",
            "                               Time Taken  \n",
            "Model                                      \n",
            "NuSVC                                0.87  \n",
            "SVC                                  0.73  \n",
            "LGBMClassifier                      30.63  \n",
            "XGBClassifier                       13.22  \n",
            "RandomForestClassifier               2.09  \n",
            "ExtraTreesClassifier                 0.68  \n",
            "AdaBoostClassifier                   4.37  \n",
            "NearestCentroid                      0.16  \n",
            "SGDClassifier                        0.39  \n",
            "RidgeClassifierCV                    0.53  \n",
            "BernoulliNB                          0.27  \n",
            "LinearDiscriminantAnalysis           0.73  \n",
            "GaussianNB                           0.26  \n",
            "PassiveAggressiveClassifier          0.33  \n",
            "LogisticRegression                   0.63  \n",
            "RidgeClassifier                      0.26  \n",
            "Perceptron                           0.22  \n",
            "BaggingClassifier                    4.40  \n",
            "KNeighborsClassifier                 0.28  \n",
            "CalibratedClassifierCV              10.08  \n",
            "LinearSVC                            2.69  \n",
            "DecisionTreeClassifier               0.83  \n",
            "ExtraTreeClassifier                  0.12  \n",
            "QuadraticDiscriminantAnalysis        2.10  \n",
            "DummyClassifier                      0.13  \n",
            "LabelSpreading                       0.57  \n",
            "LabelPropagation                     0.53  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "clf=LazyClassifier(verbose=0,ignore_warnings=True,custom_metric=None)\n",
        "models,predictions=clf.fit(X_train,X_test,y_train,y_test)\n",
        "print(models)"
      ],
      "id": "_J97NymFGAmL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ThorBNh3tbf"
      },
      "source": [
        "# SVM"
      ],
      "id": "1ThorBNh3tbf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtvwN1tC3zfg",
        "outputId": "7b3c0d04-a5a6-43f1-96ca-a6cc9a0a6435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6990595611285266\n",
            "QWK: 0.6260420284821118\n",
            "F1 Score\n",
            "Label 0  F1 Score: 0.5208333333333334\n",
            "Label 1  F1 Score: 0.7195121951219511\n",
            "Label 2  F1 Score: 0.7476635514018691\n"
          ]
        }
      ],
      "source": [
        "#clf = make_pipeline(StandardScaler(), SVC())\n",
        "clf = svm.SVC(kernel='linear', C=5)\n",
        "clf.fit(X_train, y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
        "print(\"QWK:\", cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
        "\n",
        "print(\"F1 Score\")\n",
        "for i in range(num_labels):\n",
        "  print(\"Label\",i,\" F1 Score:\", f1_score(y_test, pred,average=None)[i])\n"
      ],
      "id": "MtvwN1tC3zfg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfYUBqdzOT2n"
      },
      "source": [
        "# LogisticRegression "
      ],
      "id": "YfYUBqdzOT2n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxf0vp-RObMi",
        "outputId": "64bcfc58-9264-413a-ae2e-9d88a890c381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7115987460815048\n",
            "QWK: 0.6453706014474669\n",
            "F1 Score\n",
            "Label 0  F1 Score: 0.5057471264367817\n",
            "Label 1  F1 Score: 0.7337278106508877\n",
            "Label 2  F1 Score: 0.76056338028169\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
        "print(\"QWK:\", cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
        "\n",
        "print(\"F1 Score\")\n",
        "for i in range(num_labels):\n",
        "  print(\"Label\",i,\" F1 Score:\", f1_score(y_test, pred,average=None)[i])"
      ],
      "id": "uxf0vp-RObMi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt561bZL2BwR"
      },
      "source": [
        "\n",
        "# KNeighborsClassifier"
      ],
      "id": "lt561bZL2BwR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2J8VFdI2JSB",
        "outputId": "843b95b9-d369-4018-89da-1761eaa76efe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5574229691876751\n",
            "QWK: 0.34214769429216385\n",
            "F1 Score\n",
            "Label 0  F1 Score: 0.689655172413793\n",
            "Label 1  F1 Score: 0.2876712328767123\n",
            "Label 2  F1 Score: 0.4691358024691358\n"
          ]
        }
      ],
      "source": [
        "clf = KNeighborsClassifier(n_neighbors=9)\n",
        "clf.fit(X_train, y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
        "print(\"QWK:\", cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
        "\n",
        "print(\"F1 Score\")\n",
        "for i in range(num_labels):\n",
        "  print(\"Label\",i,\" F1 Score:\", f1_score(y_test, pred,average=None)[i])"
      ],
      "id": "h2J8VFdI2JSB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1iXgaoW3b1M"
      },
      "source": [
        "# ExtraTreesClassifier"
      ],
      "id": "C1iXgaoW3b1M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvX5RpOU3fGB",
        "outputId": "bdc446a6-a401-4364-c55e-12ddd8ffc11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5826330532212886\n",
            "QWK: 0.21158068057080126\n",
            "F1 Score\n",
            "Label 0  F1 Score: 0.717434869739479\n",
            "Label 1  F1 Score: 0.24561403508771934\n",
            "Label 2  F1 Score: 0.297029702970297\n"
          ]
        }
      ],
      "source": [
        "clf = ExtraTreesClassifier(n_estimators=50, random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
        "print(\"QWK:\", cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
        "\n",
        "print(\"F1 Score\")\n",
        "for i in range(num_labels):\n",
        "  print(\"Label\",i,\" F1 Score:\", f1_score(y_test, pred,average=None)[i])"
      ],
      "id": "pvX5RpOU3fGB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDIT18ImBFuN"
      },
      "source": [
        "# RandomForestClassifier"
      ],
      "id": "NDIT18ImBFuN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydvJn_M6BGif",
        "outputId": "61a69a50-b85a-4d89-c16d-b4ddc696ff4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5518207282913166\n",
            "QWK: 0.19820959731755927\n",
            "F1 Score\n",
            "Label 0  F1 Score: 0.7033898305084745\n",
            "Label 1  F1 Score: 0.24\n",
            "Label 2  F1 Score: 0.27350427350427353\n"
          ]
        }
      ],
      "source": [
        "clf = RandomForestClassifier(n_estimators=10,max_depth=9,random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
        "print(\"QWK:\", cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
        "\n",
        "print(\"F1 Score\")\n",
        "for i in range(num_labels):\n",
        "  print(\"Label\",i,\" F1 Score:\", f1_score(y_test, pred,average=None)[i])"
      ],
      "id": "ydvJn_M6BGif"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSmp-W9qBGD8"
      },
      "source": [
        "# LGBMClassifier"
      ],
      "id": "pSmp-W9qBGD8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86ZkPU83BHkY",
        "outputId": "9eb4661b-1dd5-4526-9b9a-579301cb955e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6833855799373041\n",
            "QWK: 0.6263627353815658\n",
            "F1 Score\n",
            "Label 0  F1 Score: 0.47727272727272735\n",
            "Label 1  F1 Score: 0.7023809523809524\n",
            "Label 2  F1 Score: 0.7383177570093457\n"
          ]
        }
      ],
      "source": [
        "\n",
        "clf = lgb.LGBMClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
        "print(\"QWK:\", cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
        "\n",
        "print(\"F1 Score\")\n",
        "for i in range(num_labels):\n",
        "  print(\"Label\",i,\" F1 Score:\", f1_score(y_test, pred,average=None)[i])"
      ],
      "id": "86ZkPU83BHkY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtElsC6VqJ0e"
      },
      "source": [
        "# XGBClassifier"
      ],
      "id": "ZtElsC6VqJ0e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1WH8XkioBem",
        "outputId": "7de31cec-99ed-456d-ea82-ca642c3a53ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5658263305322129\n",
            "QWK: 0.24501025268530363\n",
            "F1 Score\n",
            "Label 0  F1 Score: 0.7112970711297072\n",
            "Label 1  F1 Score: 0.22399999999999998\n",
            "Label 2  F1 Score: 0.32432432432432434\n"
          ]
        }
      ],
      "source": [
        "clf = XGBClassifier(learning_rate =0.05,\n",
        " n_estimators=50,\n",
        " max_depth=3,\n",
        " #min_child_weight=1,\n",
        " #gamma=0,\n",
        " subsample=0.8,\n",
        " colsample_bytree=0.8,\n",
        " objective= 'multi:softprob',\n",
        " nthread=4,\n",
        " seed=27)\n",
        "\n",
        "#xgbc = XGBClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
        "print(\"QWK:\", cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
        "\n",
        "print(\"F1 Score\")\n",
        "for i in range(num_labels):\n",
        "  print(\"Label\",i,\" F1 Score:\", f1_score(y_test, pred,average=None)[i])"
      ],
      "id": "J1WH8XkioBem"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezTJovQekX4x"
      },
      "source": [
        "\n",
        "# BaggingClassifier"
      ],
      "id": "ezTJovQekX4x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRlBaOydi-gi",
        "outputId": "99bceb7c-6484-4844-da84-c5aeb42faa10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5994397759103641\n",
            "QWK: 0.28854673081072857\n",
            "F1 Score\n",
            "Label 0  F1 Score: 0.7249999999999999\n",
            "Label 1  F1 Score: 0.28333333333333327\n",
            "Label 2  F1 Score: 0.4035087719298246\n"
          ]
        }
      ],
      "source": [
        "clf = BaggingClassifier(n_estimators=150,random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
        "print(\"QWK:\", cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
        "\n",
        "print(\"F1 Score\")\n",
        "for i in range(num_labels):\n",
        "  print(\"Label\",i,\" F1 Score:\", f1_score(y_test, pred,average=None)[i])"
      ],
      "id": "cRlBaOydi-gi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAKgSA2fheEA"
      },
      "source": [
        "# NuSVC\n"
      ],
      "id": "yAKgSA2fheEA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7w_SP6Jhfon",
        "outputId": "f1265231-19ce-4787-b81d-281852bd1d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6358543417366946\n",
            "QWK: 0.44950223899929675\n",
            "F1 Score\n",
            "Label 0  F1 Score: 0.7563805104408351\n",
            "Label 1  F1 Score: 0.3918918918918919\n",
            "Label 2  F1 Score: 0.5185185185185185\n"
          ]
        }
      ],
      "source": [
        "clf = make_pipeline(StandardScaler(), NuSVC())\n",
        "clf.fit(X_train, y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
        "print(\"QWK:\", cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
        "\n",
        "print(\"F1 Score\")\n",
        "for i in range(num_labels):\n",
        "  print(\"Label\",i,\" F1 Score:\", f1_score(y_test, pred,average=None)[i])\n"
      ],
      "id": "u7w_SP6Jhfon"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkPtEYoshebP"
      },
      "source": [
        "# LinearSVC"
      ],
      "id": "rkPtEYoshebP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnSUr-APhvII",
        "outputId": "5616992d-bf1c-4697-f592-e52d161e3e3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5126050420168067\n",
            "QWK: 0.3203675848453511\n",
            "F1 Score\n",
            "Label 0  F1 Score: 0.6277777777777778\n",
            "Label 1  F1 Score: 0.33333333333333337\n",
            "Label 2  F1 Score: 0.4642857142857143\n"
          ]
        }
      ],
      "source": [
        "clf = make_pipeline(StandardScaler(),LinearSVC(random_state=0, tol=1e-5))\n",
        "clf.fit(X_train, y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
        "print(\"QWK:\", cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
        "\n",
        "print(\"F1 Score\")\n",
        "for i in range(num_labels):\n",
        "  print(\"Label\",i,\" F1 Score:\", f1_score(y_test, pred,average=None)[i])\n"
      ],
      "id": "MnSUr-APhvII"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi7EXttWhe0P"
      },
      "source": [
        "# SGDClassifier"
      ],
      "id": "Fi7EXttWhe0P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YIjbAElhzjf",
        "outputId": "23f6a2d3-aa90-410f-8ae3-b85db24a286e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5602240896358543\n",
            "QWK: 0.3764738588359008\n",
            "F1 Score\n",
            "Label 0  F1 Score: 0.6810810810810811\n",
            "Label 1  F1 Score: 0.3957219251336898\n",
            "Label 2  F1 Score: 0.4713375796178344\n"
          ]
        }
      ],
      "source": [
        "clf = make_pipeline(StandardScaler(),SGDClassifier(max_iter=1000, tol=1e-3))\n",
        "clf.fit(X_train, y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
        "print(\"QWK:\", cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
        "\n",
        "print(\"F1 Score\")\n",
        "for i in range(num_labels):\n",
        "  print(\"Label\",i,\" F1 Score:\", f1_score(y_test, pred,average=None)[i])\n"
      ],
      "id": "8YIjbAElhzjf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAcSXEvnYhIy"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# NearestCentroid"
      ],
      "id": "TAcSXEvnYhIy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDoRUmFHP9en"
      },
      "outputs": [],
      "source": [],
      "id": "tDoRUmFHP9en"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cn4KRMjYhow",
        "outputId": "df398de5-d42f-46b0-e7e2-97e1a8d37bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5378151260504201\n",
            "QWK: 0.3163235285954561\n",
            "F1 Score\n",
            "Label 0  F1 Score: 0.6495726495726496\n",
            "Label 1  F1 Score: 0.4\n",
            "Label 2  F1 Score: 0.4574468085106383\n"
          ]
        }
      ],
      "source": [
        "clf = NearestCentroid()\n",
        "clf.fit(X_train, y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
        "print(\"QWK:\", cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
        "\n",
        "print(\"F1 Score\")\n",
        "for i in range(num_labels):\n",
        "  print(\"Label\",i,\" F1 Score:\", f1_score(y_test, pred,average=None)[i])"
      ],
      "id": "2cn4KRMjYhow"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LinearDiscriminantAnalysis"
      ],
      "metadata": {
        "id": "8dbO0eZpB2SU"
      },
      "id": "8dbO0eZpB2SU"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U7NXd_qSB4Rt"
      },
      "id": "U7NXd_qSB4Rt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LinearDiscriminantAnalysis()\n",
        "clf.fit(X_train, y_train)\n",
        "pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
        "print(\"QWK:\", cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
        "\n",
        "print(\"F1 Score\")\n",
        "for i in range(num_labels):\n",
        "  print(\"Label\",i,\" F1 Score:\", f1_score(y_test, pred,average=None)[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb4eb3ZOB6BN",
        "outputId": "425fadf5-0375-4fca-c50c-651bd0749006"
      },
      "id": "wb4eb3ZOB6BN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5098039215686274\n",
            "QWK: 0.35184188582614306\n",
            "F1 Score\n",
            "Label 0  F1 Score: 0.6263736263736265\n",
            "Label 1  F1 Score: 0.29189189189189185\n",
            "Label 2  F1 Score: 0.496969696969697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4btXUe9UjOsF"
      },
      "id": "4btXUe9UjOsF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8d4_IY8Zj2dk",
        "RM0L3PcAygBo",
        "GwGYpVBAUWzn",
        "ckT842vQUXM_",
        "VAlXVr-7Um1Y",
        "SW1orBt7GBza",
        "1ThorBNh3tbf",
        "YfYUBqdzOT2n",
        "lt561bZL2BwR",
        "C1iXgaoW3b1M",
        "NDIT18ImBFuN",
        "pSmp-W9qBGD8",
        "ZtElsC6VqJ0e",
        "ezTJovQekX4x",
        "yAKgSA2fheEA",
        "rkPtEYoshebP",
        "Fi7EXttWhe0P",
        "TAcSXEvnYhIy",
        "8dbO0eZpB2SU"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}